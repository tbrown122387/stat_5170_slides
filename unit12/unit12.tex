\def\CTeXPreproc{Created by ctex v0.2.9, don't edit!}
%\documentclass{beamer}
\documentclass[%handout,
xcolor=pdftex]{beamer}
\mode<presentation> {
  \usetheme{Warsaw}
  \setbeamercovered{transparent}
}
\let\Tiny=\tiny
\usetheme{Singapore}
\usecolortheme{dolphin}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{bm}
%\setbeamertemplate{headline}{}
\setbeamertemplate{footline}[page number]
\newcommand\Fontvi{\fontsize{9pt}{8}\selectfont}
\newcommand\Fontvii{\fontsize{7pt}{8}\selectfont}
\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}\newtheorem{proposition}{Proposition}
\title{Unit 12: Forecasting}
\author[STAT 5170: Applied Time Series, Unit 12]{Taylor R. Brown}
\institute{Department of Statistics, University of Virginia}
\date{Fall 2020} 

\AtBeginSubsection[] {
  \begin{frame}<beamer>{Outline}
    \tableofcontents[currentsection,currentsubsection]
  \end{frame}
}

\begin{document}


\frame{\titlepage}


\begin{frame}
\frametitle{Readings for Unit 12}


Textbook chapter 3.4.

\end{frame}



\begin{frame}
\frametitle{Last Unit}
\begin{enumerate}
\item ACF for MA(q)
\item ACF for Causal ARMA(p,q)
\item Partial Autocorrelation Function
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{This Unit}
\begin{enumerate}
\item Best linear predictor
\item ARMA forecasting
\end{enumerate}
\end{frame}


\begin{frame}
\frametitle{Motivation}

In this unit, we explore forecasting: predicting future values of a time series based on observed data.

\end{frame}

\section{Forecasting for Stationary Processes}
\frame{\tableofcontents[currentsection]}

\begin{frame}
\frametitle{Forecasting}

In forecasting, the goal is to predict future values of a time series, $x_{n+m}$, based on the observed data $\mathbf{x} = \{x_n, x_{n-1}, \cdots, x_1 \}$. In this unit, we assume $\{x_t\}$ is stationary.

\end{frame}

\begin{frame}
\frametitle{Conditional Expectations}

Conditional expectations are almost always the way you want to use data to forecast/predict something else. It's no different in this situation:

\begin{equation}
x_{n+m}^n = \mbox{E}(x_{n+m} | \mathbf{x}_{1:n})
\end{equation}

is the best way to forecast $m$ steps into the future with the data you have in the sense that that point minimizes mean square error $\mbox{E}\left[x_{n+m} - g(\mathbf{x}_{1:n}) \right]^2$, where $g(\mathbf{x})$ is any function of the observations.

\end{frame}

\begin{frame}
\frametitle{Forecasting}

First, we restrict our attention to predictors that are linear functions of the observations, i.e.

\begin{equation} \label{eq:BLP}
x_{n+m}^n = \alpha_0 + \sum_{j=1}^n \alpha_j x_j
\end{equation}

where $\alpha_0, \alpha_1, \cdots, \alpha_n \in \mathbb{R}$. Linear predictors of the form (\ref{eq:BLP}) that minimize the mean square prediction error are called \textbf{best linear predictors (BLP)}.\\

\vspace{5mm}
Linear prediction depends on on the second-order moments of the process, which can be estimated from the data.

\end{frame}

%\begin{frame}
%\frametitle{Projection Theorem}
%
%\begin{theorem}
%\label{projection}
%Let $\mathcal{M}$ be a closed subspace of the Hilbert space $\mathcal{H}$ and let $y$ be an element in $\mathcal{H}$. Then, $y$ can be uniquely represented as $y = \hat{y} + z$ where $\hat{y} \in \mathcal{M}$ and $z$ is orthogonal to $\mathcal{M}$. Therefore, for any $w \in \mathcal{M}$,
%
%\begin{itemize}
%\item $\lVert y-w \rVert \geq \lVert y-\hat{y} \rVert$ and
%\item $<z,w> = 0$.
%\end{itemize}
%\end{theorem}
%
%%$\hat{y}$ is the projection of $y$ on $\mathcal{M}$.
%
%\end{frame}
%
%\begin{frame}
%\frametitle{Projection Theorem}
%
%
%\end{frame}
%
%\begin{frame}
%\frametitle{Projection Theorem: Linear Prediction}
%
%Given $1, x_1, x_2, \cdots, x_n \in \{X: \mbox{E}(X^2) < \infty \}$, choose $\alpha_0, \alpha_1, \cdots, \alpha_n \in \mathbb{R}$ so that $U = \alpha_0 + \sum_{j=1}^n \alpha_j x_j$ minimizes $\mbox{E}(x_{n+m} - U)^2$.\\
%\vspace{5mm}
%
%Note that: $\mathcal{M} = \{U = \alpha_0 + \sum_{j=1}^n \alpha_j x_j: \alpha_j \in \mathbb{R} \} = \bar{sp}\{1,x_1, \cdots, x_n \}$ and $y=x_{n+m}$.
%\end{frame}
%
%\begin{frame}
%\frametitle{Projection Theorem: Linear Prediction}
%
%Let $x_{n+m}^n$ denote the best linear predictor, i.e.
%
%$$
%\lVert x_{n+m}^n - x_{n+m} \rVert^2 \leq \lVert U - x_{n+m} \rVert^2
%$$
%
%for all $U \in \mathcal{M}$. The projection theorem implies\\
%
%\vspace{40mm}
%
%\end{frame}

\begin{frame}
\frametitle{Property 3.3}

We find the $\alpha$s in $x_{n+m}^n = \alpha_0 + \sum_{j=1}^n \alpha_j x_j$ by taking the derivatives of 
$$
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right)^2\right]
$$
and setting them equal to $0$.


\end{frame}


\begin{frame}
\frametitle{Property 3.3}

We find the $\alpha$s in $x_{n+m}^n = \alpha_0 + \sum_{j=1}^n \alpha_j x_j$ we solve the following system of equations: 
\begin{align*}
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right) \right] &= 0 \\
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right)x_1 \right] &= 0 \\
\vdots&\\
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right)x_n \right] &= 0
\end{align*}
\end{frame}


\begin{frame}
\frametitle{Property 3.3}

Find $x_{n+m}^n = \alpha_0 + \sum_{j=1}^n \alpha_j x_j$ by solving: 

$$
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right)x_k \right] = 0 
$$
for $k=0,1,\ldots, n$ ($x_0 = 1$).

A few takeaways:

\begin{itemize}
\item Every time we get a new data point, we have to recalculate the prediction
\item Every time we get a new data point, there is another equation in a new system of equations
\item Prediction errors $x_{n+m} - x_{n+m}^n$ are orthogonal/uncorrelated with the prediction variables $(1,x_1, \cdots, x_n)$.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Property 3.3}

Find $x_{n+m}^n = \alpha_0 + \sum_{j=1}^n \alpha_j x_j$ by solving: 

$$
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right)x_k \right] = 0 
$$
for $k=0,1,\ldots, n$ ($x_0 = 1$).

More takeaways:

\begin{itemize}
\item We want recursive formulae: get $x_{n+m}^n$ from $x_{n-1+m}^{n-1}$.
\item We also want our recursive formulas to have bounded memory footprints.
\item Different algorithms work for different models: innovations algorithm, Durbin-Levinson, Kalman filter, particle filters, etc.
\item The book also describes algorithms that assume you have an infinite past of history.
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Property 3.3}

After simplifying the first equation
$$
E\left[ \left(x_{n+m} - \left( \alpha_0 + \sum_{j=1}^n \alpha_j x_j\right) \right) \right] = 0
$$
into $\mu = \alpha_0 + \sum_{j=1}^n \alpha_j \mu,$ we can solve for the intercept
$$
\alpha_0 = \mu\left[1 - \sum_{j=1}^n \alpha_j \right].
$$
So the general form of the forecast is 
$$
x_{n+m}^n = \mu + \sum_{j=1}^n (x_j - \mu)
$$

From now on, we can assume without loss of generality that $\mu = 0$.
\end{frame}


\begin{frame}
\frametitle{Property 3.3}

Another step, focus on one-step-ahead, and change 
$$
x_{n+1}^n = \alpha_n x_n + \alpha_{n-1}x_{n-1} + \cdots + \alpha_1 x_1
$$
into
$$
x_{n+1}^n = \phi_{n1} x_n + \phi_{n2}x_{n-1} + \cdots + \phi_{nn} x_1.
$$
Writing $\phi$ instead of $\alpha$ and reversing the subscripts is purely superficial, and is meant to suggest that the upcoming work is going to be more useful for pure AR(p) models. It turns 
$$
E\left[ \left(x_{n+1} - \left( \sum_{j=1}^n \alpha_j x_j\right) \right)x_k \right] = 0
$$
for $k=1,\ldots,n$ into 
$$
E\left[ \left(x_{n+1} - \left( \sum_{j=1}^n \phi_{nj} x_{n+1-j}\right)\right) x_{n+1-k}  \right] = 0
$$


\end{frame}


\begin{frame}
\frametitle{Property 3.3}

For $k=1,\ldots,n$, write
$$
E\left[ \left(x_{n+1} - \left( \sum_{j=1}^n \phi_{nj} x_{n+1-j}\right)\right)x_{n+1-k}  \right] = 0
$$
as
$$
\gamma_x(k) - \sum_{j=1}^n \phi_{nj}\gamma_x(k-j) = 0
$$
or ... 


\end{frame}


\begin{frame}
\frametitle{Property 3.3}
$\gamma_x(k) - \sum_{j=1}^n \phi_{nj}\gamma_x(k-j) = 0$
as
$$
\begin{bmatrix}
\gamma(0) & \gamma(1) & \cdots &\gamma(n-1) \\
\gamma(1) & \gamma(0) & \cdots & \gamma(n-2) \\
\vdots & \vdots & \ddots & \vdots \\
\gamma(n-1) & \gamma(n-2) & \cdots & \gamma(0)
\end{bmatrix}
\begin{bmatrix}
\phi_{n1} \\
\phi_{n2} \\
\vdots\\
\phi_{nn}
\end{bmatrix}
=
\begin{bmatrix}
\gamma(1) \\
\gamma(2) \\
\vdots \\
\gamma(n)
\end{bmatrix}
$$
or in shorter form
$$
\Gamma_n \phi_n = \gamma_n
$$


\end{frame}


\begin{frame}
\frametitle{Obtaining $x_{n+1}^n$ }

For each data set size $n$, we solve 
$$
\Gamma_n \phi_n = \gamma_n
$$
for $\phi_n$. The bigger the data, the more rows of that matrix. Theoretically, it's just
$$
\phi_n = \Gamma_n^{-1} \gamma_n
$$
and 
$$
x_{n+1}^n = \phi_n' x
$$
where $x = (x_n, x_{n-1}, \ldots, x_1)'$. Practically, though, naively inverting a gigantic matrix can be a deal breaker.

\end{frame}


\begin{frame}
\frametitle{Obtaining $x_{n+1}^n$ }

Once we have the forecast from the weight vector solution: $x_{n+1}^n = \phi_n' x$, we can calculate its variance as well:
\begin{align*}
E[(x_{n+1} - x_{n+1}^n)^2] &= E[(x_{n+1} - \phi_n' x)^2] \\
&= E[x_{n+1}^2] - 2 E[x_{n+1}\phi_n' x] + E[\phi_n' x \phi_n' x] \\
&= \gamma_x(0)- 2 E[x_{n+1} x']\phi_n + \phi_n' E[x x']\phi_n\\
&= \gamma_x(0)- 2 \gamma_n' \phi_n + \phi_n' \Gamma_n \phi_n\\
&= \gamma_x(0)- 2 \gamma_n' \phi_n + \phi_n' \gamma_n \\
&= \gamma_x(0)-  \gamma_n' \phi_n  \\
&= \gamma_x(0)-  \gamma_n' \Gamma_n^{-1}\gamma_n  
\end{align*}
$\gamma_n' \phi_n$ is the reduction in uncertainty and depends on how much autocorrelation you have.

\end{frame}

\begin{frame}
\frametitle{Example: AR(2)}

If we have an AR(2) $x_{n+1} = \phi_1 x_{n} + \phi_2 x_{n-1} + w_t$, let's find the predictions/forecasts for $n=2$, $n=3$, ... and see a pattern.
\newline

At $n=2$, forecasting time $3$ requires solving
$$
\Gamma_n \phi_n = \gamma_n
$$
which is just two equations:
$$
\begin{bmatrix}
\gamma_x(0) & \gamma_x(1) \\
\gamma_x(1) & \gamma_x(0)
\end{bmatrix}
\begin{bmatrix}
\phi_{21} \\
\phi_{22}
\end{bmatrix}
=
\begin{bmatrix}
\gamma(1) \\
\gamma(2)
\end{bmatrix}
$$

 
\end{frame}



\begin{frame}
\frametitle{Example: AR(2)}

In this case, it helps to write 
$$
\begin{bmatrix}
\gamma_x(0) & \gamma_x(1) \\
\gamma_x(1) & \gamma_x(0)
\end{bmatrix}
\begin{bmatrix}
\phi_{21} \\
\phi_{22}
\end{bmatrix}
=
\begin{bmatrix}
\gamma(1) \\
\gamma(2)
\end{bmatrix}
$$
as
$$
E\left[ \left(x_{n+1} - \left( \sum_{j=1}^n \phi_{nj} x_{n+1-j}\right)\right)x_{n+1-k}  \right] = 0
$$
for $k=1,2$. Clearly $\phi_{21} = \phi_1$ and $\phi_{22} = \phi_2$. 
\end{frame}


\begin{frame}
\frametitle{Example: AR(2)}

Assume now that $n > 2$. The prediction equations are now for $k=1,\ldots,n$
$$
E\left[ \left(x_{n+1} - \left( \sum_{j=1}^n \phi_{nj} x_{n+1-j}\right)\right)x_{n+1-k}  \right] = 0
$$
Clearly 
$$
\begin{bmatrix}
\phi_{n1} \\
\phi_{n2} \\
\phi_{n3} \\
\vdots \\
\phi_{nn}
\end{bmatrix}
=
\begin{bmatrix}
\phi_{1} \\
\phi_{2} \\
0 \\
\vdots \\
0
\end{bmatrix}
$$ 
is the solution to these.
\end{frame}

\begin{frame}
\frametitle{Example: AR(p)}

Assume at time $t \ge p$ we want to predict/forecast time $t+1$ with an AR(p) model for which we know the parameters $\phi_1, \ldots, \phi_p$. We would just use the previous $p$ time points $t, t-1, \ldots, t-p+1$ and use $\phi_1 x_t + \cdots \phi_p x_{t-p+1}$ as a prediction.
\newline

We won't derive the {\bf Durbin-Levinson} algorithm, but that's essentially what it's getting at. It's recursive, and the calculations don't get more difficult at each time point, but it's really only useful for pure AR(p) models.

\end{frame}


\begin{frame}
\frametitle{The Innovations Algorithm}

The book also mentions the {\bf Innovations Algorithm}, which is more useful for pure MA(q) models, and can be extended to ARMA(p,q) models. The idea is to write predictions in terms of 
$$
\hat{x}_{n+1}^n = \sum_{j=1}^n \theta_{nj} ( x_{n+1-j} - \hat{x}_{n+1-j})
$$
instead of 
$$
\hat{x}_{n+1}^n = \sum_{j=1}^n \phi_{nj} x_{n+1-j}.
$$
The $x_{n+1-j} - \hat{x}_{n+1-j}$ are called the innovations, and are kind of like $w_{n+1-j}$.
\end{frame}








\end{document} 